# 第1章 机器学习基础

[![机器学习基础_首页](https://github.com/apachecn/AiLearning/raw/master/docs/ml/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%A6%96%E9%A1%B5.jpg)](https://github.com/apachecn/AiLearning/blob/master/docs/ml/img/机器学习基础-首页.jpg)

## 机器学习 概述

`机器学习(Machine Learning,ML)` 是使用计算机来彰显数据背后的真实含义，它为了把无序的数据转换成有用的信息。是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。 它是人工智能的核心，是使计算机具有智能的根本途径，其应用遍及人工智能的各个领域，它主要使用归纳、综合而不是演绎。

1. 海量的数据
2. 获取有用的信息

## 机器学习 研究意义

机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在经验学习中改善具体算法的性能”。 “机器学习是对能通过经验自动改进的计算机算法的研究”。 “机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。” 一种经常引用的英文定义是: A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.

机器学习已经有了十分广泛的应用，例如: 数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信用卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人运用。

## 机器学习 场景

- 例如: 识别动物猫
  - 模式识别（官方标准）: 人们通过大量的经验，得到结论，从而判断它就是猫。
  - 机器学习（数据学习）: 人们通过阅读进行学习，观察它会叫、小眼睛、两只耳朵、四条腿、一条尾巴，得到结论，从而判断它就是猫。
  - 深度学习（深入数据）: 人们通过深入了解它，发现它会'喵喵'的叫、与同类的猫科动物很类似，得到结论，从而判断它就是猫。（深度学习常用领域: 语音识别、图像识别）
- 模式识别（pattern recognition）: 模式识别是最古老的（作为一个术语而言，可以说是很过时的）。
  - 我们把环境与客体统称为“模式”，识别是对模式的一种认知，是如何让一个计算机程序去做一些看起来很“智能”的事情。
  - 通过融于智慧和直觉后，通过构建程序，识别一些事物，而不是人，例如: 识别数字。
- 机器学习（machine learning）: 机器学习是最基础的（当下初创公司和研究实验室的热点领域之一）。
  - 在90年代初，人们开始意识到一种可以更有效地构建模式识别算法的方法，那就是用数据（可以通过廉价劳动力采集获得）去替换专家（具有很多图像方面知识的人）。
  - “机器学习”强调的是，在给计算机程序（或者机器）输入一些数据后，它必须做一些事情，那就是学习这些数据，而这个学习的步骤是明确的。
  - 机器学习（Machine Learning）是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身性能的学科。
- 深度学习（deep learning）: 深度学习是非常崭新和有影响力的前沿领域，我们甚至不会去思考-后深度学习时代。
  - 深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。
- 参考地址:
  - [深度学习 vs 机器学习 vs 模式识别](http://www.csdn.net/article/2015-03-24/2824301)
  - [深度学习 百科资料](http://baike.baidu.com/link?url=76P-uA4EBrC3G-I__P1tqeO7eoDS709Kp4wYuHxc7GNkz_xn0NxuAtEohbpey7LUa2zUQLJxvIKUx4bnrEfOmsWLKbDmvG1PCoRkJisMTQka6-QReTrIxdYY3v93f55q)

> 机器学习已应用于多个领域，远远超出大多数人的想象，横跨: 计算机科学、工程技术和统计学等多个学科。

- 搜索引擎: 根据你的搜索点击，优化你下次的搜索结果,是机器学习来帮助搜索引擎判断哪个结果更适合你（也判断哪个广告更适合你）。
- 垃圾邮件: 会自动的过滤垃圾广告邮件到垃圾箱内。
- 超市优惠券: 你会发现，你在购买小孩子尿布的时候，售货员会赠送你一张优惠券可以兑换6罐啤酒。
- 邮局邮寄: 手写软件自动识别寄送贺卡的地址。
- 申请贷款: 通过你最近的金融活动信息进行综合评定，决定你是否合格。

## 机器学习 组成

### 主要任务

- 分类（classification）: 将实例数据划分到合适的类别中。
  - 应用实例: 判断网站是否被黑客入侵（二分类 ），手写数字的自动识别（多分类）
- 回归（regression）: 主要用于预测数值型数据。
  - 应用实例: 股票价格波动的预测，房屋价格的预测等。

### 监督学习（supervised learning）

- 必须确定目标变量的值，以便机器学习算法可以发现特征和目标变量之间的关系。在监督学习中，给定一组数据，我们知道正确的输出结果应该是什么样子，并且知道在输入和输出之间有着一个特定的关系。 (包括: 分类和回归)

- 样本集: 训练数据 + 测试数据

  - 训练样本 = 特征(feature) + 目标变量(label: 分类-离散值/回归-连续值)
  - 特征通常是训练样本集的列，它们是独立测量得到的。
  - 目标变量: 目标变量是机器学习预测算法的测试结果。
    - 在分类算法中目标变量的类型通常是标称型(如: 真与假)，而在回归算法中通常是连续型(如: 1~100)。

- 监督学习需要注意的问题:

  - 偏置方差权衡
  - 功能的复杂性和数量的训练数据
  - 输入空间的维数
  - 噪声中的输出值

- ```
  知识表示
  ```

  :

  - 可以采用规则集的形式【例如: 数学成绩大于90分为优秀】
  - 可以采用概率分布的形式【例如: 通过统计分布发现，90%的同学数学成绩，在70分以下，那么大于70分定为优秀】
  - 可以使用训练样本集中的一个实例【例如: 通过样本集合，我们训练出一个模型实例，得出 年轻，数学成绩中高等，谈吐优雅，我们认为是优秀】

### 非监督学习（unsupervised learning）

- 在机器学习，无监督学习的问题是，在未加标签的数据中，试图找到隐藏的结构。因为提供给学习者的实例是未标记的，因此没有错误或报酬信号来评估潜在的解决方案。
- 无监督学习是密切相关的统计数据密度估计的问题。然而无监督学习还包括寻求，总结和解释数据的主要特点等诸多技术。在无监督学习使用的许多方法是基于用于处理数据的数据挖掘方法。
- 数据没有类别信息，也不会给定目标值。
- 非监督学习包括的类型:
  - 聚类: 在无监督学习中，将数据集分成由类似的对象组成多个类的过程称为聚类。
  - 密度估计: 通过样本分布的紧密程度，来估计与分组的相似性。
  - 此外，无监督学习还可以减少数据特征的维度，以便我们可以使用二维或三维图形更加直观地展示数据信息。

### 强化学习

这个算法可以训练程序做出某一决定。程序在某一情况下尝试所有的可能行动，记录不同行动的结果并试着找出最好的一次尝试来做决定。 属于这一类算法的有马尔可夫决策过程。

### 训练过程

![img](https://raw.githubusercontent.com/apachecn/AiLearning/master/docs/ml/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B.jpg)

### 算法汇总

![ml_algorithm.jpg](https://github.com/apachecn/AiLearning/blob/master/docs/ml/img/ml_algorithm.jpg?raw=true)

## 机器学习 使用

> 选择算法需要考虑的两个问题

1. 算法场景
   - 预测明天是否下雨，因为可以用历史的天气情况做预测，所以选择监督学习算法
   - 给一群陌生的人进行分组，但是我们并没有这些人的类别信息，所以选择无监督学习算法、通过他们身高、体重等特征进行处理。
2. 需要收集或分析的数据是什么

> 举例

![机器学习基础-选择算法.jpg](https://github.com/apachecn/AiLearning/blob/master/docs/ml/img/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80-%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95.jpg?raw=true)

> 机器学习 开发流程

1. 收集数据: 收集样本数据
2. 准备数据: 注意数据的格式
3. 分析数据: 为了确保数据集中没有垃圾数据；
   - 如果是算法可以处理的数据格式或可信任的数据源，则可以跳过该步骤；
   - 另外该步骤需要人工干预，会降低自动化系统的价值。
4. 训练算法: [机器学习算法核心]如果使用无监督学习算法，由于不存在目标变量值，则可以跳过该步骤
5. 测试算法: [机器学习算法核心]评估算法效果
6. 使用算法: 将机器学习算法转为应用程序

## 机器学习 数学基础

- 微积分
- 统计学/概率论
- 线性代数

## 机器学习 工具

### Python语言

1. 可执行伪代码
2. Python比较流行: 使用广泛、代码范例多、丰富模块库，开发周期短
3. Python语言的特色: 清晰简练、易于理解
4. Python语言的缺点: 唯一不足的是性能问题
5. Python相关的库
   - 科学函数库: `SciPy`、`NumPy`(底层语言: C和Fortran)
   - 绘图工具库: `Matplotlib`
   - 数据分析库 `Pandas`

### 数学工具

- Matlab

## 附: 机器学习专业术语

- 模型（model）: 计算机层面的认知
- 学习算法（learning algorithm），从数据中产生模型的方法
- 数据集（data set）: 一组记录的合集
- 示例（instance）: 对于某个对象的描述
- 样本（sample）: 也叫示例
- 属性（attribute）: 对象的某方面表现或特征
- 特征（feature）: 同属性
- 属性值（attribute value）: 属性上的取值
- 属性空间（attribute space）: 属性张成的空间
- 样本空间/输入空间（samplespace）: 同属性空间
- 特征向量（feature vector）: 在属性空间里每个点对应一个坐标向量，把一个示例称作特征向量
- 维数（dimensionality）: 描述样本参数的个数（也就是空间是几维的）
- 学习（learning）/训练（training）: 从数据中学得模型
- 训练数据（training data）: 训练过程中用到的数据
- 训练样本（training sample）:训练用到的每个样本
- 训练集（training set）: 训练样本组成的集合
- 假设（hypothesis）: 学习模型对应了关于数据的某种潜在规则
- 真相（ground-truth）:真正存在的潜在规律
- 学习器（learner）: 模型的另一种叫法，把学习算法在给定数据和参数空间的实例化
- 预测（prediction）: 判断一个东西的属性
- 标记（label）: 关于示例的结果信息，比如我是一个“好人”。
- 样例（example）: 拥有标记的示例
- 标记空间/输出空间（label space）: 所有标记的集合
- 分类（classification）: 预测是离散值，比如把人分为好人和坏人之类的学习任务
- 回归（regression）: 预测值是连续值，比如你的好人程度达到了0.9，0.6之类的
- 二分类（binary classification）: 只涉及两个类别的分类任务
- 正类（positive class）: 二分类里的一个
- 反类（negative class）: 二分类里的另外一个
- 多分类（multi-class classification）: 涉及多个类别的分类
- 测试（testing）: 学习到模型之后对样本进行预测的过程
- 测试样本（testing sample）: 被预测的样本
- 聚类（clustering）: 把训练集中的对象分为若干组
- 簇（cluster）: 每一个组叫簇
- 监督学习（supervised learning）: 典范--分类和回归
- 无监督学习（unsupervised learning）: 典范--聚类
- 未见示例（unseen instance）: “新样本“，没训练过的样本
- 泛化（generalization）能力: 学得的模型适用于新样本的能力
- 分布（distribution）: 样本空间的全体样本服从的一种规律
- 独立同分布（independent and identically distributed，简称i,i,d.）:获得的每个样本都是独立地从这个分布上采样获得的。

## 机器学习基础补充

### 数据集的划分

- 训练集（Training set） —— 学习样本数据集，通过匹配一些参数来建立一个模型，主要用来训练模型。类比考研前做的解题大全。
- 验证集（validation set） —— 对学习出来的模型，调整模型的参数，如在神经网络中选择隐藏单元数。验证集还用来确定网络结构或者控制模型复杂程度的参数。类比 考研之前做的模拟考试。
- 测试集（Test set） —— 测试训练好的模型的分辨能力。类比 考研。这次真的是一考定终身。

### 模型拟合程度

- 欠拟合（Underfitting）: 模型没有很好地捕捉到数据特征，不能够很好地拟合数据，对训练样本的一般性质尚未学好。类比，光看书不做题觉得自己什么都会了，上了考场才知道自己啥都不会。
- 过拟合（Overfitting）: 模型把训练样本学习“太好了”，可能把一些训练样本自身的特性当做了所有潜在样本都有的一般性质，导致泛化能力下降。类比，做课后题全都做对了，超纲题也都认为是考试必考题目，上了考场还是啥都不会。

通俗来说，欠拟合和过拟合都可以用一句话来说，欠拟合就是: “你太天真了！”，过拟合就是: “你想太多了！”。

### 常见的模型指标

- 正确率 —— 提取出的正确信息条数 / 提取出的信息条数
- 召回率 —— 提取出的正确信息条数 / 样本中的信息条数
- F 值 —— 正确率 * 召回率 * 2 / （正确率 + 召回率）（F值即为正确率和召回率的调和平均值）

举个例子如下:

举个例子如下: 某池塘有 1400 条鲤鱼，300 只虾，300 只乌龟。现在以捕鲤鱼为目的。撒了一张网，逮住了 700 条鲤鱼，200 只 虾， 100 只乌龟。那么这些指标分别如下: 正确率 = 700 / (700 + 200 + 100) = 70% 召回率 = 700 / 1400 = 50% F 值 = 70% * 50% * 2 / (70% + 50%) = 58.3%

### 模型

- 分类问题 —— 说白了就是将一些未知类别的数据分到现在已知的类别中去。比如，根据你的一些信息，判断你是高富帅，还是穷屌丝。评判分类效果好坏的三个指标就是上面介绍的三个指标: 正确率，召回率，F值。
- 回归问题 —— 对数值型连续随机变量进行预测和建模的监督学习算法。回归往往会通过计算 误差（Error）来确定模型的精确性。
- 聚类问题 —— 聚类是一种无监督学习任务，该算法基于数据的内部结构寻找观察样本的自然族群（即集群）。聚类问题的标准一般基于距离: 簇内距离（Intra-cluster Distance） 和 簇间距离（Inter-cluster Distance） 。簇内距离是越小越好，也就是簇内的元素越相似越好；而簇间距离越大越好，也就是说簇间（不同簇）元素越不相同越好。一般的，衡量聚类问题会给出一个结合簇内距离和簇间距离的公式。

下面这个图可以比较直观地展示出来:

![ml_add_1.jpg](https://github.com/apachecn/AiLearning/blob/master/docs/ml/img/ml_add_1.jpg?raw=true)

### 特征工程的一些小东西

- 特征选择 —— 也叫特征子集选择（FSS，Feature Subset Selection）。是指从已有的 M 个特征（Feature）中选择 N 个特征使得系统的特定指标最优化，是从原始特征中选择出一些最有效特征以降低数据集维度的过程，是提高算法性能的一个重要手段，也是模式识别中关键的数据预处理步骤。
- 特征提取 —— 特征提取是计算机视觉和图像处理中的一个概念。它指的是使用计算机提取图像信息，决定每个图像的点是否属于一个图像特征。特征提取的结果是把图像上的点分为不同的子集，这些子集往往属于孤立的点，连续的曲线或者连续的区域。

下面给出一个特征工程的图:

![img](https://raw.githubusercontent.com/apachecn/AiLearning/master/docs/ml/img/ml_add_2.jpg)

### 其他

- Learning rate —— 学习率，通俗地理解，可以理解为步长，步子大了，很容易错过最佳结果。就是本来目标尽在咫尺，可是因为我迈的步子很大，却一下子走过了。步子小了呢，就是同样的距离，我却要走很多很多步，这样导致训练的耗时费力还不讨好。
- 一个总结的知识点很棒的链接 : https://zhuanlan.zhihu.com/p/25197792

## 线性代数



### **标量**

一个标量就是一个单独的数，一般用小写的的变量名称表示。

### **向量**

一个向量就是一列数，这些数是有序排列的。用过次序中的索引，我们可以确定每个单独的数。通常会赋予向量粗体的小写名称。当我们需要明确表示向量中的元素时，我们会将元素排
列成一个方括号包围的纵柱：

![img](https://pic2.zhimg.com/80/v2-824f0739982920b1502ca01588d35e21_720w.jpg)

我们可以把向量看作空间中的点，每个元素是不同的坐标轴上的坐标。

### **矩阵**

矩阵是二维数组，其中的每一个元素被两个索引而非一个所确定。我们通常会赋予矩阵粗体的大写变量名称，比如A。 如果一个实数矩阵高度为m，宽度为n，那么我们说**![[公式]](https://www.zhihu.com/equation?tex=A%5Cepsilon+R%5E%7Bm%5Ctimes+n%7D+)。**

![img](https://pic3.zhimg.com/80/v2-dd3017aa861f53973da40d860ec93732_720w.jpg)

矩阵这东西在机器学习中就不要太重要了！实际上，如果我们现在有N个用户的数据，每条数据含有M个特征，那其实它对应的就是一个N*M的矩阵呀；再比如，一张图由16*16的像素点组成，那这就是一个16*16的矩阵了。现在才发现，我们大一学的矩阵原理原来这么的有用！要是当时老师讲课的时候先普及一下，也不至于很多同学学矩阵的时候觉得莫名其妙了。

### **张量**

几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。

例如，可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。将这张图用张量表示出来，就是最下方的那张表格：

![img](https://pic1.zhimg.com/80/v2-c0c16793d4662bfcdd7e112030096f94_720w.jpg)

其中表的横轴表示图片的宽度值，这里只截取0~319；表的纵轴表示图片的高度值，这里只截取0~4；表格中每个方格代表一个像素点，比如第一行第一列的表格数据为[1.0,1.0,1.0]，代表的就是RGB三原色在图片的这个位置的取值情况（即R=1.0，G=1.0，B=1.0）。

当然我们还可以将这一定义继续扩展，即：我们可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据。

张量在深度学习中是一个很重要的概念，因为它是一个深度学习框架中的一个核心组件，后续的所有运算和优化算法几乎都是基于张量进行的。

### **范数**

有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用被称为范数(norm) 的函数衡量矩阵大小。Lp 范数如下：

![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%7C+%5Cleft%7C+x+%5Cright%7C+%5Cright%7C+_%7Bp%7D%5E%7B%7D+%3D%5Cleft%28+%5Csum_%7Bi%7D%5E%7B%7D%7B%5Cleft%7C+x_%7Bi%7D+%5Cright%7C+%5E%7Bp%7D+%7D+%5Cright%29+_%7B%7D%5E%7B%5Cfrac%7B1%7D%7Bp%7D+%7D+)

所以：

L1范数![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%7C+%5Cleft%7C+x+%5Cright%7C+%5Cright%7C+)：为x向量各个元素绝对值之和；

L2范数![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%7C+%5Cleft%7C+x+%5Cright%7C+%5Cright%7C+_%7B2%7D+)：为x向量各个元素平方和的开方。

这里先说明一下，**在机器学习中，L1范数和L2范数很常见，主要用在损失函数中起到一个限制模型参数复杂度的作用**，至于为什么要限制模型的复杂度，这又涉及到机器学习中常见的过拟合问题。具体的概念在后续文章中会有详细的说明和推导，大家先记住：这个东西很重要，实际中经常会涉及到，面试中也常会被问到！！！

### **特征分解**

许多数学对象可以通过将它们分解成多个组成部分。特征分解是使用最广的矩阵分解之一，即将矩阵分解成一组**特征向量**和**特征值**。

方阵A的特征向量是指与A相乘后相当于对该向量进行缩放的非零向量![[公式]](https://www.zhihu.com/equation?tex=%5Cnu+)：

![[公式]](https://www.zhihu.com/equation?tex=A%5Cnu+%3D%5Clambda+%5Cnu+)

标量![[公式]](https://www.zhihu.com/equation?tex=%5Clambda+)被称为这个特征向量对应的特征值。

使用特征分解去分析矩阵A时，得到特征向量构成的矩阵V和特征值构成的向量![[公式]](https://www.zhihu.com/equation?tex=%5Clambda+)，我们可以重新将A写作：

![[公式]](https://www.zhihu.com/equation?tex=A%3DVdiag%5Cleft%28+%5Clambda+%5Cright%29+V%5E%7B-1%7D+)

### **奇异值分解（Singular Value Decomposition，SVD）**

矩阵的特征分解是有前提条件的，那就是只有对可对角化的矩阵才可以进行特征分解。但实际中很多矩阵往往不满足这一条件，甚至很多矩阵都不是方阵，就是说连矩阵行和列的数目都不相等。这时候怎么办呢？人们将矩阵的特征分解进行推广，得到了一种叫作“矩阵的奇异值分解”的方法，简称SVD。通过奇异分解，我们会得到一些类似于特征分解的信息。

它的具体做法是将一个普通矩阵分解为奇异向量和奇异值。比如将矩阵A分解成三个矩阵的乘积：

![[公式]](https://www.zhihu.com/equation?tex=A%3DUDV%5E%7BT%7D+)

假设**A是一个m![[公式]](https://www.zhihu.com/equation?tex=%5Ctimes+)n矩阵**，那么**U是一个m![[公式]](https://www.zhihu.com/equation?tex=%5Ctimes+)m矩阵**，**D是一个m![[公式]](https://www.zhihu.com/equation?tex=%5Ctimes+)n矩阵**，**V是一个n![[公式]](https://www.zhihu.com/equation?tex=%5Ctimes+)n矩阵**。

这些矩阵每一个都拥有特殊的结构，其中U和V都是正交矩阵，D是对角矩阵（注意，D不一定是方阵）。对角矩阵D对角线上的元素被称为矩阵A的奇异值。矩阵U的列向量被称为**左奇异向量**，矩阵V 的列向量被称**右奇异向量**。

SVD最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。另外，SVD可用于推荐系统中。

### **Moore-Penrose伪逆**

对于非方矩阵而言，其逆矩阵没有定义。假设在下面问题中，我们想通过矩阵A的左逆B来求解线性方程：

![[公式]](https://www.zhihu.com/equation?tex=Ax%3Dy)

等式两边同时左乘左逆B后，得到：

![[公式]](https://www.zhihu.com/equation?tex=x%3DBy)

是否存在唯一的映射将A映射到B取决于问题的形式。

如果矩阵A的行数大于列数，那么上述方程可能没有解；如果矩阵A的行数小于列数，那么上述方程可能有多个解。

Moore-Penrose伪逆使我们能够解决这种情况，矩阵A的伪逆定义为：

![img](https://pic4.zhimg.com/80/v2-1581c66947da5c30172f4ef80dd0b70f_720w.jpg)

但是计算伪逆的实际算法没有基于这个式子，而是使用下面的公式：

![img](https://pic2.zhimg.com/80/v2-2845b623dc537e3bae0db22c4938e9c1_720w.jpg)

其中，矩阵U，D 和V 是矩阵A奇异值分解后得到的矩阵。对角矩阵D 的伪逆D+ 是其非零元素取倒之后再转置得到的。

### **几种常用的距离**

上面大致说过， 在机器学习里，我们的运算一般都是基于向量的，一条用户具有100个特征，那么他对应的就是一个100维的向量，通过计算两个用户对应向量之间的距离值大小，有时候能反映出这两个用户的相似程度。这在后面的KNN算法和K-means算法中很明显。

设有两个n维变量![[公式]](https://www.zhihu.com/equation?tex=A%3D%5Cleft%5B+x_%7B11%7D%2C+x_%7B12%7D%2C...%2Cx_%7B1n%7D+%5Cright%5D+)和![[公式]](https://www.zhihu.com/equation?tex=B%3D%5Cleft%5B+x_%7B21%7D+%2Cx_%7B22%7D+%2C...%2Cx_%7B2n%7D+%5Cright%5D+)，则一些常用的距离公式定义如下：

**1、曼哈顿距离**

曼哈顿距离也称为城市街区距离，数学定义如下：

![[公式]](https://www.zhihu.com/equation?tex=d_%7B12%7D+%3D%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%7B%5Cleft%7C+x_%7B1k%7D-x_%7B2k%7D+%5Cright%7C+%7D+)

曼哈顿距离的Python实现：

```python
from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print sum(abs(vector1-vector2))
```

**2、欧氏距离**

欧氏距离其实就是L2范数，数学定义如下：

![[公式]](https://www.zhihu.com/equation?tex=d_%7B12%7D+%3D%5Csqrt%7B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%7B%5Cleft%28+x_%7B1k%7D+-x_%7B2k%7D+%5Cright%29+%5E%7B2%7D+%7D+%7D+)

欧氏距离的Python实现：

```python
from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print sqrt((vector1-vector2)*(vector1-vector2).T)
```

**3、闵可夫斯基距离**

从严格意义上讲，闵可夫斯基距离不是一种距离，而是一组距离的定义：

![[公式]](https://www.zhihu.com/equation?tex=d_%7B12%7D+%3D%5Csqrt%5Bp%5D%7B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%7B%5Cleft%28+x_%7B1k%7D+-x_%7B2k%7D+%5Cright%29+%5E%7Bp%7D+%7D+%7D+)

实际上，当p=1时，就是曼哈顿距离；当p=2时，就是欧式距离。



**4、切比雪夫距离**

切比雪夫距离就是![[公式]](https://www.zhihu.com/equation?tex=L_%7B%5Cvarpi%7D+)，即无穷范数，数学表达式如下：

![[公式]](https://www.zhihu.com/equation?tex=d_%7B12%7D+%3Dmax%5Cleft%28+%5Cleft%7C+x_%7B1k%7D-x_%7B2k%7D+%5Cright%7C+%5Cright%29+)

切比雪夫距离额Python实现如下：

```python
from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print sqrt(abs(vector1-vector2).max)
```

**5、夹角余弦**

夹角余弦的取值范围为[-1,1]，可以用来衡量两个向量方向的差异；夹角余弦越大，表示两个向量的夹角越小；当两个向量的方向重合时，夹角余弦取最大值1；当两个向量的方向完全相反时，夹角余弦取最小值-1。

机器学习中用这一概念来衡量样本向量之间的差异，其数学表达式如下：

![[公式]](https://www.zhihu.com/equation?tex=cos%5Ctheta+%3D%5Cfrac%7BAB%7D%7B%5Cleft%7C+A+%5Cright%7C+%5Cleft%7CB+%5Cright%7C+%7D+%3D%5Cfrac%7B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%7Bx_%7B1k%7Dx_%7B2k%7D+%7D+%7D%7B%5Csqrt%7B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%7Bx_%7B1k%7D%5E%7B2%7D+%7D+%7D+%5Csqrt%7B%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%7Bx_%7B2k%7D%5E%7B2%7D+%7D+%7D+%7D+)

夹角余弦的Python实现：

```python
from numpy import *
vector1 = mat([1,2,3])
vector2 = mat([4,5,6])
print dot(vector1,vector2)/(linalg.norm(vector1)*linalg.norm(vector2))
```

**6、汉明距离**

汉明距离定义的是两个字符串中不相同位数的数目。

例如：字符串‘1111’与‘1001’之间的汉明距离为2。

信息编码中一般应使得编码间的汉明距离尽可能的小。

汉明距离的Python实现：

```python
from numpy import *
matV = mat([1,1,1,1],[1,0,0,1])
smstr = nonzero(matV[0]-matV[1])
print smstr
```

**7、杰卡德相似系数**

两个集合A和B的交集元素在A和B的并集中所占的比例称为两个集合的杰卡德相似系数，用符号J(A,B)表示，数学表达式为：

![[公式]](https://www.zhihu.com/equation?tex=J%5Cleft%28+A%2CB+%5Cright%29+%3D%5Cfrac%7B%5Cleft%7C+A%5Ccap+B%5Cright%7C+%7D%7B%5Cleft%7CA%5Ccup+B+%5Cright%7C+%7D+)

杰卡德相似系数是衡量两个集合的相似度的一种指标。一般可以将其用在衡量样本的相似度上。

**8、杰卡德距离**

与杰卡德相似系数相反的概念是杰卡德距离，其定义式为：

![[公式]](https://www.zhihu.com/equation?tex=J_%7B%5Csigma%7D+%3D1-J%5Cleft%28+A%2CB+%5Cright%29+%3D%5Cfrac%7B%5Cleft%7C+A%5Ccup+B+%5Cright%7C+-%5Cleft%7C+A%5Ccap+B+%5Cright%7C+%7D%7B%5Cleft%7C+A%5Ccup+B+%5Cright%7C+%7D+)

杰卡德距离的Python实现：

```python
from numpy import *
import scipy.spatial.distance as dist
matV = mat([1,1,1,1],[1,0,0,1])
print dist.pdist(matV,'jaccard')
```

## **概率**



### **为什么使用概率？**

概率论是用于表示不确定性陈述的数学框架，即它是对事物不确定性的度量。

在人工智能领域，我们主要以两种方式来使用概率论。首先，概率法则告诉我们AI系统应该如何推理，所以我们设计一些算法来计算或者近似由概率论导出的表达式。其次，我们可以用概率和统计从理论上分析我们提出的AI系统的行为。

计算机科学的许多分支处理的对象都是完全确定的实体，但机器学习却大量使用概率论。实际上如果你了解机器学习的工作原理你就会觉得这个很正常。因为机器学习大部分时候处理的都是不确定量或随机量。

### **随机变量**

随机变量可以随机地取不同值的变量。我们通常用小写字母来表示随机变量本身，而用带数字下标的小写字母来表示随机变量能够取到的值。例如，![[公式]](https://www.zhihu.com/equation?tex=x_%7B1%7D+) 和![[公式]](https://www.zhihu.com/equation?tex=x_%7B2%7D+) 都是随机变量X可能的取值。

对于向量值变量，我们会将随机变量写成X，它的一个值为![[公式]](https://www.zhihu.com/equation?tex=x)。就其本身而言，一个随机变量只是对可能的状态的描述；它必须伴随着一个概率分布来指定每个状态的可能性。

随机变量可以是离散的或者连续的。

### **概率分布**

给定某随机变量的取值范围，概率分布就是导致该随机事件出现的可能性。

从机器学习的角度来看，概率分布就是符合随机变量取值范围的某个对象属于某个类别或服从某种趋势的可能性。

### 条件概率

很多情况下，我们感兴趣的是某个事件在给定其它事件发生时出现的概率，这种概率叫条件概率。

我们将给定![[公式]](https://www.zhihu.com/equation?tex=X%3Dx)时![[公式]](https://www.zhihu.com/equation?tex=Y%3Dy)发生的概率记为![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+Y%3Dy%7CX%3Dx+%5Cright%29+)，这个概率可以通过下面的公式来计算：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+Y%3Dy%7CX%3Dx+%5Cright%29+%3D%5Cfrac%7BP%5Cleft%28+Y%3Dy%2CX%3Dx+%5Cright%29+%7D%7BP%5Cleft%28+X%3Dx+%5Cright%29+%7D+)

### **贝叶斯公式**

先看看什么是“先验概率”和“后验概率”，以一个例子来说明：

假设某种病在人群中的发病率是0.001，即1000人中大概会有1个人得病，则有： **P(患病) = 0.1%**；即：在没有做检验之前，我们预计的患病率为**P(患病)=0.1%**，这个就叫作**"先验概率"**。

再假设现在有一种该病的检测方法，其检测的准确率为**95%**；即：如果真的得了这种病，该检测法有**95%**的概率会检测出阳性，但也有**5%**的概率检测出阴性；或者反过来说，但如果没有得病，采用该方法有**95%**的概率检测出阴性，但也有**5%**的概率检测为阳性。用概率条件概率表示即为：**P(显示阳性|患病)=95%**

现在我们想知道的是：在做完检测显示为阳性后，某人的患病率**P(患病|显示阳性)**，这个其实就称为**"后验概率"。**

而这个叫贝叶斯的人其实就是为我们提供了一种可以**利用先验概率计算后验概率**的方法，我们将其称为**“贝叶斯公式”。**

这里先了解**条件概率公式**：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+B%7CA+%5Cright%29%3D%5Cfrac%7BP%5Cleft%28+AB+%5Cright%29%7D%7BP%5Cleft%28+A+%5Cright%29%7D+%2C+P%5Cleft%28+A%7CB+%5Cright%29%3D%5Cfrac%7BP%5Cleft%28+AB+%5Cright%29%7D%7BP%5Cleft%28+B+%5Cright%29%7D)

由条件概率可以得到**乘法公式**：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+AB+%5Cright%29%3DP%5Cleft%28+B%7CA+%5Cright%29P%5Cleft%28+A+%5Cright%29%3DP%5Cleft%28+A%7CB+%5Cright%29P%5Cleft%28+B+%5Cright%29)

将条件概率公式和乘法公式结合可以得到：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+B%7CA+%5Cright%29%3D%5Cfrac%7BP%5Cleft%28+A%7CB+%5Cright%29%5Ccdot+P%5Cleft%28+B+%5Cright%29%7D%7BP%5Cleft%28+A+%5Cright%29%7D)

再由**全概率公式**：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+A+%5Cright%29%3D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7BP%5Cleft%28+A%7CB_%7Bi%7D+%5Cright%29+%5Ccdot+P%5Cleft%28+B_%7Bi%7D%5Cright%29%7D+)

代入可以得到**贝叶斯公式**：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+B_%7Bi%7D%7CA+%5Cright%29%3D%5Cfrac%7BP%5Cleft%28+A%7CB_%7Bi%7D+%5Cright%29%5Ccdot+P%5Cleft%28+B_%7Bi%7D+%5Cright%29%7D%7B%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7BP%5Cleft%28+A%7CB_%7Bi%7D+%5Cright%29+%5Ccdot+P%5Cleft%28+B_%7Bi%7D%5Cright%29%7D+%7D)

在这个例子里就是：

![img](https://pic2.zhimg.com/80/v2-e3e7a3aa9fb146d662591612b3cac465_720w.jpg)

贝叶斯公式贯穿了机器学习中随机问题分析的全过程。从文本分类到概率图模型，其基本分类都是贝叶斯公式。

### **期望**

在概率论和统计学中，数学期望是试验中每次可能结果的概率乘以其结果的总和。它是最基本的数学特征之一，反映随机变量平均值的大小。

假设X是一个离散随机变量，其可能的取值有：![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+x_%7B1%7D+%2Cx_%7B2%7D+%2C......%2Cx_%7Bn%7D+%5Cright%5C%7D+)，各个取值对应的概率取值为：![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+x_%7Bk%7D+%5Cright%29+%2C+k%3D1%2C2%2C......%2Cn)，则其数学期望被定义为：

![[公式]](https://www.zhihu.com/equation?tex=E%5Cleft%28X+%5Cright%29+%3D%5Csum_%7Bk%3D1%7D%5E%7Bn%7D%7Bx_%7Bk%7D+P%5Cleft%28+x_%7Bk%7D+%5Cright%29+%7D+)

假设X是一个连续型随机变量，其概率密度函数为![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+x+%5Cright%29+)则其数学期望被定义为：

![[公式]](https://www.zhihu.com/equation?tex=E%5Cleft%28+x+%5Cright%29+%3D%5Cint_%7B-%5Cvarpi+%7D%5E%7B%2B%5Cvarpi+%7D+xf%5Cleft%28+x+%5Cright%29+dx)

### **方差**

概率中，方差用来衡量随机变量与其数学期望之间的偏离程度；统计中的方差为样本方差，是各个样本数据分别与其平均数之差的平方和的平均数。数学表达式如下：

![[公式]](https://www.zhihu.com/equation?tex=Var%5Cleft%28+x+%5Cright%29+%3DE%5Cleft%5C%7B+%5Cleft%5B+x-E%5Cleft%28+x+%5Cright%29+%5Cright%5D+%5E%7B2%7D+%5Cright%5C%7D+%3DE%5Cleft%28+x%5E%7B2%7D+%5Cright%29+-%5Cleft%5B+E%5Cleft%28+x+%5Cright%29+%5Cright%5D+%5E%7B2%7D+)

### **协方差**

在概率论和统计学中，协方差被用于衡量两个随机变量X和Y之间的总体误差。数学定义式为：

![[公式]](https://www.zhihu.com/equation?tex=Cov%5Cleft%28+X%2CY+%5Cright%29+%3DE%5Cleft%5B+%5Cleft%28+X-E%5Cleft%5B+X+%5Cright%5D+%5Cright%29+%5Cleft%28+Y-E%5Cleft%5B+Y+%5Cright%5D+%5Cright%29+%5Cright%5D+%3DE%5Cleft%5B+XY+%5Cright%5D+-E%5Cleft%5B+X+%5Cright%5D+E%5Cleft%5B+Y+%5Cright%5D+)

### **常见分布函数**

**1）0-1分布**

0-1分布是单个二值型离散随机变量的分布，其概率分布函数为：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+X%3D1+%5Cright%29+%3Dp)![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+X%3D0+%5Cright%29+%3D1-p)

**2）几何分布**

几何分布是离散型概率分布，其定义为：在n次伯努利试验中，试验k次才得到第一次成功的机率。即：前k-1次皆失败，第k次成功的概率。其概率分布函数为：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+X%3Dk+%5Cright%29+%3D%5Cleft%28+1-p+%5Cright%29+%5E%7Bk-1%7D+p)

性质：

![[公式]](https://www.zhihu.com/equation?tex=E%5Cleft%28+X+%5Cright%29+%3D%5Cfrac%7B1%7D%7Bp%7D+)![[公式]](https://www.zhihu.com/equation?tex=Var%5Cleft%28+X+%5Cright%29+%3D%5Cfrac%7B1-p%7D%7Bp%5E%7B2%7D+%7D+)

**3）二项分布**

二项分布即重复n次伯努利试验，各次试验之间都相互独立，并且每次试验中只有两种可能的结果，而且这两种结果发生与否相互对立。如果每次试验时，事件发生的概率为p，不发生的概率为1-p，则n次重复独立试验中发生k次的概率为：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+X%3Dk+%5Cright%29+%3DC_%7Bn%7D%5E%7Bk%7D+p%5E%7Bk%7D+%5Cleft%28+1-p+%5Cright%29+%5E%7Bn-k%7D+)

性质：

![[公式]](https://www.zhihu.com/equation?tex=E%5Cleft%28+X+%5Cright%29+%3Dnp)![[公式]](https://www.zhihu.com/equation?tex=Var%5Cleft%28+X+%5Cright%29+%3Dnp%5Cleft%28+1-p+%5Cright%29+)

**4）高斯分布**

高斯分布又叫正态分布，其曲线呈钟型，两头低，中间高，左右对称因其曲线呈钟形，如下图所示：

![img](https://pic1.zhimg.com/80/v2-a0811acc8ab121a3ad8f2e37ff6c37cc_720w.jpg)

若随机变量X服从一个数学期望为![[公式]](https://www.zhihu.com/equation?tex=%5Cmu+)，方差为![[公式]](https://www.zhihu.com/equation?tex=%5Csigma+%5E%7B2%7D+)的正态分布，则我们将其记为：![[公式]](https://www.zhihu.com/equation?tex=N%5Cleft%28+%5Cmu+%2C%5Csigma%5E%7B2%7D+%5Cright%29+)。其期望值![[公式]](https://www.zhihu.com/equation?tex=%5Cmu+)决定了正态分布的位置，其标准差![[公式]](https://www.zhihu.com/equation?tex=%5Csigma+)（方差的开方）决定了正态分布的幅度。

**5）指数分布**

指数分布是事件的时间间隔的概率，它的一个重要特征是无记忆性。例如：如果某一元件的寿命的寿命为T，已知元件使用了t小时，它总共使用至少t+s小时的条件概率，与从开始使用时算起它使用至少s小时的概率相等。下面这些都属于指数分布：

- 婴儿出生的时间间隔
- 网站访问的时间间隔
- 奶粉销售的时间间隔

指数分布的公式可以从泊松分布推断出来。如果下一个婴儿要间隔时间t，就等同于t之内没有任何婴儿出生，即：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+X%5Cgeq+t+%5Cright%29+%3DP%5Cleft%28+N%5Cleft%28+t+%5Cright%29+%3D0+%5Cright%29+%3D%5Cfrac%7B%5Cleft%28+%5Clambda+t+%5Cright%29+%5E%7B0%7D%5Ccdot+e%5E%7B-%5Clambda+t%7D+%7D%7B0%21%7D%3De%5E%7B-%5Clambda+t%7D+)

则：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+X%5Cleq+t+%5Cright%29+%3D1-P%5Cleft%28+X%5Cgeq+t+%5Cright%29+%3D1-e%5E%7B-%5Clambda+t%7D+)

如：接下来15分钟，会有婴儿出生的概率为：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+X%5Cleq+%5Cfrac%7B1%7D%7B4%7D+%5Cright%29+%3D1-e%5E%7B-3%5Ccdot+%5Cfrac%7B1%7D%7B4%7D+%7D+%5Capprox+0.53)

指数分布的图像如下：

![img](https://pic3.zhimg.com/80/v2-a58c37c481e032bbb53ff17113754ef6_720w.jpg)

**6）泊松分布**

日常生活中，大量事件是有固定频率的，比如：

- 某医院平均每小时出生3个婴儿
- 某网站平均每分钟有2次访问
- 某超市平均每小时销售4包奶粉

它们的特点就是，我们可以预估这些事件的总数，但是没法知道具体的发生时间。已知平均每小时出生3个婴儿，请问下一个小时，会出生几个？有可能一下子出生6个，也有可能一个都不出生，这是我们没法知道的。

**泊松分布就是描述某段时间内，事件具体的发生概率。**其概率函数为：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+N%5Cleft%28+t+%5Cright%29+%3Dn+%5Cright%29+%3D%5Cfrac%7B%5Cleft%28+%5Clambda+t+%5Cright%29+%5E%7Bn%7De%5E%7B-%5Clambda+t%7D+%7D%7Bn%21%7D+)

其中：

P表示概率，N表示某种函数关系，t表示时间，n表示数量，1小时内出生3个婴儿的概率，就表示为 P(N(1) = 3) ；λ 表示事件的频率。

还是以上面医院平均每小时出生3个婴儿为例，则![[公式]](https://www.zhihu.com/equation?tex=%5Clambda+%3D3)；

那么，接下来两个小时，一个婴儿都不出生的概率可以求得为：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+N%5Cleft%282+%5Cright%29+%3D0+%5Cright%29+%3D%5Cfrac%7B%5Cleft%28+3%5Ccdot+2+%5Cright%29+%5E%7Bo%7D+%5Ccdot+e%5E%7B-3%5Ccdot+2%7D+%7D%7B0%21%7D+%5Capprox+0.0025)

同理，我们可以求接下来一个小时，至少出生两个婴儿的概率：

![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+N%5Cleft%28+1+%5Cright%29+%5Cgeq+2+%5Cright%29+%3D1-P%5Cleft%28+N%5Cleft%28+1+%5Cright%29%3D0+%5Cright%29+-+P%5Cleft%28+N%5Cleft%28+1+%5Cright%29%3D1+%5Cright%29%5Capprox+0.8)

【注】上面的指数分布和泊松分布参考了阮一峰大牛的博客：“泊松分布和指数分布：10分钟教程”，在此说明，也对其表示感谢！

### **Lagrange乘子法**

对于一般的求极值问题我们都知道，求导等于0就可以了。但是如果我们不但要求极值，还要求一个满足一定约束条件的极值，那么此时就可以构造Lagrange函数，其实就是**把约束项添加到原函数上，然后对构造的新函数求导**。

对于一个要求极值的函数![[公式]](https://www.zhihu.com/equation?tex=f%5Cleft%28+x%2Cy+%5Cright%29+)，图上的蓝圈就是这个函数的等高图，就是说 ![[公式]](https://www.zhihu.com/equation?tex=f%5Cleft%28+x%2Cy+%5Cright%29+%3Dc_%7B1%7D+%2Cc_%7B2%7D+%2C...%2Cc_%7Bn%7D+)分别代表不同的数值(每个值代表一圈，等高图)，我要找到一组![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28+x%2Cy+%5Cright%29+)，使它的![[公式]](https://www.zhihu.com/equation?tex=c_%7Bi%7D+)值越大越好，但是这点必须满足约束条件![[公式]](https://www.zhihu.com/equation?tex=g%5Cleft%28+x%2Cy+%5Cright%29+)（在黄线上）。

![img](https://pic3.zhimg.com/80/v2-806fd987177e32a33e698caa74d69942_720w.jpg)

也就是说![[公式]](https://www.zhihu.com/equation?tex=f%28x%2Cy%29)和![[公式]](https://www.zhihu.com/equation?tex=g%28x%2Cy%29)相切，或者说它们的梯度▽![[公式]](https://www.zhihu.com/equation?tex=f)和▽![[公式]](https://www.zhihu.com/equation?tex=g)平行，因此它们的梯度（偏导）成倍数关系；那我么就假设为![[公式]](https://www.zhihu.com/equation?tex=%5Clambda+)倍，然后把约束条件加到原函数后再对它求导，其实就等于满足了下图上的式子。

在**支持向量机模型（SVM）**的推导中一步很关键的就是利用拉格朗日对偶性将原问题转化为对偶问题。

### **最大似然估计**

**最大似然**也称为最大概似估计，即：在“模型已定，参数θ未知”的情况下，通过观测数据估计未知参数θ 的一种思想或方法。

**其基本思想是**： 给定样本取值后，该样本最有可能来自参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+)为何值的总体。即：寻找![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Ctheta+%7D+_%7BML%7D+)使得观测到样本数据的可能性最大。

举个例子，假设我们要统计全国人口的身高，首先假设这个身高服从服从正态分布，但是该分布的均值与方差未知。由于没有足够的人力和物力去统计全国每个人的身高，但是可以通过采样（所有的采样要求都是独立同分布的），获取部分人的身高，然后通过最大似然估计来获取上述假设中的正态分布的均值与方差。

**求极大似然函数估计值的一般步骤：**

- 1、写出似然函数；

![img](https://pic3.zhimg.com/80/v2-84eef0a858928f3cc28fd03bd7286b3a_720w.jpg)

- 2、对似然函数取对数；
- 3、两边同时求导数；
- 4、令导数为0解出似然方程。

在机器学习中也会经常见到极大似然的影子。比如后面的**逻辑斯特回归模型（LR）**，其核心就是构造对数损失函数后运用极大似然估计。

## **信息论**

### **熵**

如果一个随机变量X的可能取值为![[公式]](https://www.zhihu.com/equation?tex=X%3D%5Cleft%5C%7B+x_%7B1%7D%2Cx_%7B2%7D+%2C.....%2Cx_%7Bn%7D+%5Cright%5C%7D+)，其概率分布为![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28+X%3Dx_%7Bi%7D+%5Cright%29+%3Dp_%7Bi%7D+%2Ci%3D1%2C2%2C.....%2Cn)，则随机变量X的熵定义为**H(X)**：

![[公式]](https://www.zhihu.com/equation?tex=H%5Cleft%28+X+%5Cright%29+%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7BP%5Cleft%28+x_%7Bi%7D+%5Cright%29+logP%5Cleft%28+x_%7Bi%7D+%5Cright%29+%7D+%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7BP%5Cleft%28+x_%7Bi%7D+%5Cright%29+%5Cfrac%7B1%7D%7BlogP%5Cleft%28+x_%7Bi%7D+%5Cright%29+%7D+%7D+)

### **联合熵**

两个随机变量X和Y的联合分布可以形成联合熵，定义为联合自信息的数学期望，它是二维随机变量XY的不确定性的度量，用**H(X,Y)**表示：

![[公式]](https://www.zhihu.com/equation?tex=H%5Cleft%28+X%2CY+%5Cright%29+%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%7BP%5Cleft%28+x_%7Bi%7D+%2Cy_%7Bj%7D+%5Cright%29%7D+logP%5Cleft%28+x_%7Bi%7D%2Cy_%7Bj%7D+%5Cright%29+%7D+)

### **条件熵**

在随机变量X发生的前提下，随机变量Y发生新带来的熵，定义为Y的条件熵，用**H(Y|X)**表示：

![[公式]](https://www.zhihu.com/equation?tex=H%5Cleft%28Y%7CX+%5Cright%29+%3D-%5Csum_%7Bx%2Cy%7D%5E%7B%7D%7BP%5Cleft%28+x%2Cy+%5Cright%29+logP%5Cleft%28+y%7Cx+%5Cright%29+%7D+)

条件熵用来衡量在已知随机变量X的条件下，随机变量Y的不确定性。

实际上，熵、联合熵和条件熵之间存在以下关系：

![[公式]](https://www.zhihu.com/equation?tex=H%5Cleft%28+Y%7CX+%5Cright%29+%3DH%5Cleft%28+X%2CY%5Cright%29+-H%5Cleft%28+X+%5Cright%29+)

推导过程如下：

![img](https://pic3.zhimg.com/80/v2-adc57281fa54b0702906a8c42cbd7e5a_720w.jpg)

其中：

- 第二行推到第三行的依据是边缘分布P(x)等于联合分布P(x,y)的和；
- 第三行推到第四行的依据是把公因子logP(x)乘进去，然后把x,y写在一起；
- 第四行推到第五行的依据是：因为两个sigma都有P(x,y)，故提取公因子P(x,y)放到外边，然后把里边的-（log P(x,y) - log P(x)）写成- log (P(x,y) / P(x) ) ；
- 第五行推到第六行的依据是：P(x,y) = P(x) * P(y|x)，故P(x,y) / P(x) = P(y|x)。

### **相对熵**

相对熵又称互熵、交叉熵、KL散度、信息增益，是描述两个概率分布P和Q差异的一种方法，记为**D(P||Q)**。在信息论中，D(P||Q)表示当用概率分布Q来拟合真实分布P时，产生的信息损耗，其中P表示真实分布，Q表示P的拟合分布。

对于一个离散随机变量的两个概率分布P和Q来说，它们的相对熵定义为：

![[公式]](https://www.zhihu.com/equation?tex=D%5Cleft%28+P%7C%7CQ+%5Cright%29+%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7BP%5Cleft%28+x_%7Bi%7D+%5Cright%29+log%5Cfrac%7BP%5Cleft%28+x_%7Bi%7D+%5Cright%29+%7D%7BQ%5Cleft%28+x_%7Bi%7D+%5Cright%29+%7D+%7D+)

注意：D(P||Q) ≠ D(Q||P)

相对熵又称**KL散度( Kullback–Leibler divergence)**，KL散度也是一个机器学习中常考的概念。

### **互信息**

两个随机变量X，Y的互信息定义为X，Y的联合分布和各自独立分布乘积的相对熵称为互信息，用**I(X,Y)**表示。互信息是信息论里一种有用的信息度量方式，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性。

![[公式]](https://www.zhihu.com/equation?tex=I%5Cleft%28+X%2CY+%5Cright%29+%3D%5Csum_%7Bx%5Cin+X%7D%5E%7B%7D%7B%5Csum_%7By%5Cin+Y%7D%5E%7B%7D%7BP%5Cleft%28+x%2Cy+%5Cright%29+%7D+log%5Cfrac%7BP%5Cleft%28+x%2Cy+%5Cright%29+%7D%7BP%5Cleft%28+x+%5Cright%29+P%5Cleft%28+y+%5Cright%29+%7D+%7D+)

互信息、熵和条件熵之间存在以下关系： ![[公式]](https://www.zhihu.com/equation?tex=H%5Cleft%28+Y%7CX+%5Cright%29+%3DH%5Cleft%28+Y+%5Cright%29+-I%5Cleft%28+X%2CY+%5Cright%29+)

推导过程如下：

![img](https://pic3.zhimg.com/80/v2-6f41bffde009999cbc370f7f38cab092_720w.jpg)

通过上面的计算过程发现有：**H(Y|X) = H(Y) - I(X,Y)**，又由前面条件熵的定义有：**H(Y|X) = H(X,Y) - H(X)**，于是有**I(X,Y)= H(X) + H(Y) - H(X,Y)**，此结论被多数文献作为**互信息的定义**。

